# LLM Model Configuration
# This file contains configuration for different LLM providers and models

providers:
  anthropic:
    api_key: "${ANTHROPIC_API_KEY}"
    base_url: "https://api.anthropic.com"
    models:
      claude-haiku-4-5:
        name: "claude-haiku-4-5-20251001"
        max_tokens: 4096
        temperature: 0.1
        cost_per_token: 0.0008
      claude-sonnet-4-5:
        name: "claude-sonnet-4-5-20250929"
        max_tokens: 4096
        temperature: 0.1
        cost_per_token: 0.003
      claude-opus-4-1:
        name: "claude-opus-4-1-20250805"
        max_tokens: 4096
        temperature: 0.1
        cost_per_token: 0.015

  openrouter:
    api_key: "${OPENROUTER_API_KEY}"
    base_url: "https://openrouter.ai/api/v1"
    models:
      claude-3-haiku:
        name: "anthropic/claude-3.5-haiku"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0005
      claude-3-sonnet:
        name: "anthropic/claude-4.5-sonnet"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.003
      llama-2-70b:
        name: "meta-llama/llama-2-70b-chat"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0009
      mistral-7b:
        name: "mistralai/mistral-7b-instruct"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0002
      gemma-7b:
        name: "google/gemma-7b-it"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0001
      llama-3-8b:
        name: "meta-llama/llama-3-8b-instruct"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0001

  openai:
    api_key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    models:
      gpt-4:
        name: "gpt-4"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.03
      gpt-3.5-turbo:
        name: "gpt-3.5-turbo"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.002

  local:
    base_url: "http://localhost:11434"
    models:
      llama2:
        name: "llama2"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0
      mistral:
        name: "mistral"
        max_tokens: 4000
        temperature: 0.1
        cost_per_token: 0.0

# Default configurations
defaults:
  provider: "openrouter"
  model: "claude-3-haiku"
  max_tokens: 4000
  temperature: 0.1

# Validation-specific model settings
validation:
  # Model used for citation validation
  citation_model: "claude-3-haiku"
  # Model used for context analysis
  context_model: "claude-3-sonnet"
  # Model used for confidence scoring
  scoring_model: "claude-3-haiku"

# Article generation settings
generation:
  # Model used for article generation (switched to cheaper model)
  article_model: "claude-3-haiku"  # Was claude-3-sonnet - 83% cost reduction
  # Model used for knowledge base querying
  query_model: "claude-3-haiku"
  # Model used for citation extraction
  extraction_model: "claude-3-haiku"
